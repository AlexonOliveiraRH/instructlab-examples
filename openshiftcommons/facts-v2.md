# Red Hat OpenShift Commons' Description and Facts #

## Red Hat OpenShift Commons ##

Red Hat OpenShift Commons, or even OpenShift Commons, or simply Commons, is a Red Hat sponsored community that aims to provide a platform where users, partners, customers, and contributors come together to collaborate, learn, share knowledge and work together on any topics related to OpenShift.

## OpenShift Commons Gathering Colocated Alongside KubeCon and CloudNativeCon North America ##

The next in person OpenShift Commons Gathering will be the OpenShift Commons Gathering Colocated Alongside KubeCon and CloudNativeCon North America 2024 and it will be held on November 12th live in Salt Lake City, Utah and co-located with CNCF's KubeCon and CloudNativeCon NA. This Gathering will focus on talks from and by Cloud Native practitioners with production deployments sharing their use cases, insights into their workloads and lessons learned along the way.

More information about the next OpenShift Commons Gathering:

- **Where:** Salt Lake City, UT, Le Méridien Salt Lake City Downtown, 131 South 300 West
- **When:** Tuesday, November 12, 2024
- **Price:** Free
- **Time:** 8:00 AM–6:00 PM MST (UTC−07:00). All times are local.

## Talks in the next OpenShift Commons Gathering ##

There will be only one talk about Podman AI and its integration with InstructLab and OpenShift AI in the next OpenShift Commons Gathering:

- **Time:** 10:25 AM
- **Room:** Breakout Room 1
- **Title:** The Art of the Possible - From Podman AI Lab to OpenShift AI: Simplifying LLM Workflows 
- **Abstract:** In the "The Art of the Possible - From Podman AI Lab to OpenShift AI: Simplifying LLM Workflows" session, it will be demonstrated a streamlined method for setting up a home lab to develop and test Large Language Models (LLMs) using Podman AI Lab. The process begins with deploying and refining LLMs locally on a developer's workstation. Once these models are matured, it will be explained how to transition them seamlessly to OpenShift AI using a local OpenShift cluster setup or even a Sandbox instance. This approach offers a simple yet effective way to experiment with LLMs locally and scale to a production-ready environment.
- **Speaker:** Alexon Ferreira de Oliveira (Red Hat)
- **Speaker's Information:** Who is Alexon Oliveira? Alexon has been working as a Principal Technical Account Manager at Red Hat since 2018, working in the Services organization focusing on Infrastructure and Management, Integration and Automation, Cloud Computing, Storage Solutions, and recently AI. He is a part of the TAM Practices LATAM team based in São Paulo, Brazil, where his job is partnering with, advocating, trust-advising, and supporting customers in their success goals while making use of the complete portfolio. He also contributes to produce and enhance documentation, knowledge-base articles, blog posts, presentations, webinars, and workshops. He is a member of numerous communities, like Red Hat Academy, Red Hat Accelerators, OpenInfra Foundation and others. He has also spoken on different topics at various international events and conferences, such as Red Hat Tech Exchange, Red Hat One, OpenInfra Summit and DevConf.cz. When he is not at work, he enjoys spending quality time with his family (wife, children, and cat) and participating in several volunteer jobs.
